{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen(/Users/adamhannachi/DeepLearning/DeDeepLearning_GO/DeepLearning_GO/golois.so, 0x0002): tried: '/Users/adamhannachi/DeepLearning/DeDeepLearning_GO/DeepLearning_GO/golois.so' (slice is not valid mach-o file), '/System/Volumes/Preboot/Cryptexes/OS/Users/adamhannachi/DeepLearning/DeDeepLearning_GO/DeepLearning_GO/golois.so' (no such file), '/Users/adamhannachi/DeepLearning/DeDeepLearning_GO/DeepLearning_GO/golois.so' (slice is not valid mach-o file)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgolois\u001b[39;00m  \u001b[38;5;66;03m# Librairie C++ fournie (jeu de donn√©es Go)\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m layers, regularizers\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n",
      "\u001b[31mImportError\u001b[39m: dlopen(/Users/adamhannachi/DeepLearning/DeDeepLearning_GO/DeepLearning_GO/golois.so, 0x0002): tried: '/Users/adamhannachi/DeepLearning/DeDeepLearning_GO/DeepLearning_GO/golois.so' (slice is not valid mach-o file), '/System/Volumes/Preboot/Cryptexes/OS/Users/adamhannachi/DeepLearning/DeDeepLearning_GO/DeepLearning_GO/golois.so' (no such file), '/Users/adamhannachi/DeepLearning/DeDeepLearning_GO/DeepLearning_GO/golois.so' (slice is not valid mach-o file)"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# üì¶ IMPORTS G√âN√âRAUX\n",
    "# ================================================================\n",
    "import os, gc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from google.colab import files  # pour le t√©l√©chargement auto\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import golois  # Librairie C++ fournie (jeu de donn√©es Go)\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow import keras\n",
    "import gc\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from google.colab import files\n",
    "\n",
    "\n",
    "\n",
    "# Utilitaires internes (si tu veux factoriser ton code)\n",
    "# from utils.model_blocks import dw_conv_block, se_block, attention_residual_block\n",
    "# from utils.training_utils import moving_average, save_plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mod√®le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Param√®tres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# ‚öôÔ∏è PARAM√àTRES G√âN√âRAUX + NOMENCLATURE STANDARDIS√âE\n",
    "# ================================================================\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from datetime import datetime\n",
    "\n",
    "# -------------------------------\n",
    "# üîπ Informations g√©n√©rales\n",
    "# -------------------------------\n",
    "trainer = \"AdamH\"                      # üî∏ ton identifiant / pr√©nom (modifiable)\n",
    "base_title = \"Model4B_AttentionHybrid\" # üî∏ nom du mod√®le / projet\n",
    "date_str = datetime.now().strftime(\"%Y%m%d\")  # üìÖ date format YYYYMMDD\n",
    "\n",
    "# -------------------------------\n",
    "# üîπ Hyperparam√®tres principaux\n",
    "# -------------------------------\n",
    "planes = 31\n",
    "moves = 361\n",
    "N = 10000\n",
    "epochs = 500\n",
    "batch = 128\n",
    "filters = 110\n",
    "se_reduction = 10\n",
    "value_dense_units = 56\n",
    "l2_reg = 1e-4\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# -------------------------------\n",
    "# üîπ Nomenclature du mod√®le (automatique)\n",
    "# -------------------------------\n",
    "model_name = f\"{date_str}_{trainer}_{base_title}_ep{epochs}_bs{batch}_lr{learning_rate}_l2reg{l2_reg}\"\n",
    "print(f\"üßæ Nom du mod√®le g√©n√©r√© : {model_name}\")\n",
    "\n",
    "# -------------------------------\n",
    "# üîπ Dossier de sortie\n",
    "# -------------------------------\n",
    "output_root = \"output\"\n",
    "output_dir = os.path.join(output_root, model_name)\n",
    "\n",
    "# Supprime le dossier s'il existe d√©j√† (run propre)\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"üìÇ Dossier de sortie cr√©√© : {output_dir}\")\n",
    "\n",
    "# -------------------------------\n",
    "# üîπ Allocation des tenseurs factices (√† remplacer par vraies donn√©es)\n",
    "# -------------------------------\n",
    "input_data = np.zeros((N, 19, 19, planes), dtype=np.float32)\n",
    "policy      = np.zeros((N, moves), dtype=np.float32)\n",
    "value       = np.zeros((N,), dtype=np.float32)\n",
    "end         = np.zeros((N, 19, 19, 2), dtype=np.float32)\n",
    "groups      = np.zeros((N, 19, 19, 1), dtype=np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D√©finition du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# üß† D√âFINITION DU MOD√àLE - ATTENTION HYBRID (<100k params)\n",
    "# ================================================================\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "# üîπ BLOCS AVANC√âS\n",
    "# ---------------------\n",
    "def dw_conv_block(x, filters, l2_reg, stride=1):\n",
    "    \"\"\"Depthwise separable convolution (MobileNet style)\"\"\"\n",
    "    x = layers.SeparableConv2D(\n",
    "        filters, 3, strides=stride, padding='same', activation='relu',\n",
    "        depthwise_regularizer=regularizers.l2(l2_reg),\n",
    "        pointwise_regularizer=regularizers.l2(l2_reg)\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    return x\n",
    "\n",
    "def se_block(x, reduction=8):\n",
    "    \"\"\"Squeeze & Excitation attention block\"\"\"\n",
    "    c = int(x.shape[-1])\n",
    "    se = layers.GlobalAveragePooling2D()(x)\n",
    "    se = layers.Dense(c // reduction, activation='relu')(se)\n",
    "    se = layers.Dense(c, activation='sigmoid')(se)\n",
    "    se = layers.Reshape((1, 1, c))(se)\n",
    "    return layers.Multiply()([x, se])\n",
    "\n",
    "def attention_residual_block(x, filters, l2_reg, reduction):\n",
    "    \"\"\"Residual block combinant attention et convolutions s√©parables\"\"\"\n",
    "    shortcut = x\n",
    "    x = dw_conv_block(x, filters, l2_reg)\n",
    "    x = dw_conv_block(x, filters, l2_reg)\n",
    "    x = se_block(x, reduction=reduction)\n",
    "    x = layers.Add()([shortcut, x])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "# ---------------------\n",
    "# üîπ CONSTRUCTION DU MOD√àLE\n",
    "# ---------------------\n",
    "inp = keras.Input(shape=(19, 19, planes), name='board')\n",
    "\n",
    "x = layers.Conv2D(filters, 1, activation='relu', padding='same',\n",
    "                  kernel_regularizer=regularizers.l2(l2_reg))(inp)\n",
    "x = layers.BatchNormalization()(x)\n",
    "\n",
    "for i in range(3):\n",
    "    x = attention_residual_block(x, filters, l2_reg, reduction=se_reduction)\n",
    "    if i % 2 == 1:\n",
    "        x = layers.Dropout(0.10)(x)\n",
    "\n",
    "# --- Policy Head ---\n",
    "p = layers.Conv2D(1, 1, activation='relu', padding='same',\n",
    "                  kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "p = layers.Flatten()(p)\n",
    "policy_head = layers.Activation('softmax', name='policy')(p)\n",
    "\n",
    "# --- Value Head ---\n",
    "v = layers.GlobalAveragePooling2D()(x)\n",
    "v = layers.Dense(value_dense_units, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(l2_reg))(v)\n",
    "v = layers.Dropout(0.15)(v)\n",
    "value_head = layers.Dense(1, activation='sigmoid', name='value',\n",
    "                          kernel_regularizer=regularizers.l2(l2_reg))(v)\n",
    "\n",
    "# ---------------------\n",
    "# üîπ COMPILATION\n",
    "# ---------------------\n",
    "model = keras.Model(inputs=inp, outputs=[policy_head, value_head])\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-4, clipnorm=1.0)\n",
    "policy_loss = keras.losses.CategoricalCrossentropy(label_smoothing=0.05)\n",
    "value_loss  = keras.losses.Huber(delta=0.1)\n",
    "top5 = keras.metrics.TopKCategoricalAccuracy(k=5, name='top5')\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss={'policy': policy_loss, 'value': value_loss},\n",
    "    loss_weights={'policy': 1.0, 'value': 1.0},\n",
    "    metrics={'policy': ['categorical_accuracy', top5], 'value': ['mae']}\n",
    ")\n",
    "\n",
    "total_params = model.count_params()\n",
    "print(f\"‚úÖ {model_name} pr√™t : {total_params} param√®tres totaux (<100k attendu)\\n\")\n",
    "assert total_params < 100_000, f\"‚ùå Trop de param√®tres : {total_params} (limite <100k)\"\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# üèãÔ∏è‚Äç‚ôÇÔ∏è ENTRA√éNEMENT DU MOD√àLE\n",
    "# ================================================================\n",
    "\n",
    "# üîπ Callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(output_dir, f\"{model_name}_best.keras\"),\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.CSVLogger(\n",
    "        os.path.join(output_dir, f\"{model_name}_training_log.csv\"),\n",
    "        append=False\n",
    "    )\n",
    "]\n",
    "\n",
    "# üîπ Historique d'entra√Ænement\n",
    "history_all = {\n",
    "    \"loss\": [], \"policy_acc\": [], \"policy_top5\": [], \"value_mae\": [],\n",
    "    \"val_loss\": [], \"val_policy_acc\": [], \"val_policy_top5\": [], \"val_value_mae\": [], \"val_epochs\": []\n",
    "}\n",
    "\n",
    "# üîπ Boucle d'entra√Ænement principale\n",
    "with open(os.path.join(output_dir, f\"{model_name}_results.txt\"), \"w\") as f:\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"\\n--- Epoch {epoch}/{epochs} ---\")\n",
    "        golois.getBatch(input_data, policy, value, end, groups, epoch * N)\n",
    "        do_val = (epoch % 5 == 0) or (epoch == epochs)\n",
    "        validation_data = None\n",
    "        if do_val:\n",
    "            golois.getValidation(input_data, policy, value, end)\n",
    "            validation_data = (input_data, [policy, value])\n",
    "\n",
    "        hist = model.fit(\n",
    "            input_data, [policy, value],\n",
    "            epochs=1, batch_size=batch, verbose=1,\n",
    "            validation_data=validation_data,\n",
    "            callbacks=callbacks if do_val else None\n",
    "        )\n",
    "\n",
    "        history_all[\"loss\"].append(hist.history[\"loss\"][0])\n",
    "        history_all[\"policy_acc\"].append(hist.history[\"policy_categorical_accuracy\"][0])\n",
    "        history_all[\"policy_top5\"].append(hist.history[\"policy_top5\"][0])\n",
    "        history_all[\"value_mae\"].append(hist.history[\"value_mae\"][0])\n",
    "\n",
    "        if do_val:\n",
    "            history_all[\"val_epochs\"].append(epoch)\n",
    "            history_all[\"val_loss\"].append(hist.history[\"val_loss\"][0])\n",
    "            history_all[\"val_policy_acc\"].append(hist.history[\"val_policy_categorical_accuracy\"][0])\n",
    "            history_all[\"val_policy_top5\"].append(hist.history[\"val_policy_top5\"][0])\n",
    "            history_all[\"val_value_mae\"].append(hist.history[\"val_value_mae\"][0])\n",
    "            f.write(f\"Epoch {epoch}: val_loss={history_all['val_loss'][-1]:.4f}, \"\n",
    "                    f\"val_acc={history_all['val_policy_acc'][-1]:.4f}, \"\n",
    "                    f\"val_top5={history_all['val_policy_top5'][-1]:.4f}, \"\n",
    "                    f\"val_value_mae={history_all['val_value_mae'][-1]:.4f}\\n\")\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            gc.collect()\n",
    "\n",
    "print(f\"\\n‚úÖ Entra√Ænement termin√© pour {model_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sauvegarde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# üíæ SAUVEGARDE + üìä VISUALISATION + üìà LISSAGE (dans /output)\n",
    "# ================================================================\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from google.colab import files\n",
    "\n",
    "# üîπ Cr√©ation du dossier (si jamais supprim√© ou manquant)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# üîπ Sauvegarde du mod√®le (.h5 & .keras)\n",
    "h5_path = os.path.join(output_dir, f\"{model_name}.h5\")\n",
    "keras_path = os.path.join(output_dir, f\"{model_name}.keras\")\n",
    "hist_json = os.path.join(output_dir, \"history.json\")\n",
    "\n",
    "model.save(h5_path)\n",
    "model.save(keras_path)\n",
    "print(f\"‚úÖ Mod√®les sauvegard√©s :\\n - {h5_path}\\n - {keras_path}\")\n",
    "\n",
    "# üîπ Sauvegarde de l‚Äôhistorique complet\n",
    "with open(hist_json, \"w\") as f:\n",
    "    json.dump(history_all, f)\n",
    "print(f\"üìò Historique sauvegard√© : {hist_json}\")\n",
    "\n",
    "# üîπ T√©l√©chargement automatique du mod√®le sur Colab\n",
    "files.download(h5_path)\n",
    "print(f\"üì• T√©l√©chargement automatique du mod√®le {h5_path} termin√© ‚úÖ\")\n",
    "\n",
    "# ================================================================\n",
    "# üìä VISUALISATION DES COURBES\n",
    "# ================================================================\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "\n",
    "def save_plot(fig, name):\n",
    "    \"\"\"Sauvegarde une figure dans le dossier du mod√®le\"\"\"\n",
    "    fig.savefig(os.path.join(output_dir, name), dpi=300, bbox_inches='tight')\n",
    "\n",
    "epochs_range = range(1, len(history_all[\"loss\"]) + 1)\n",
    "\n",
    "# --- 1Ô∏è‚É£ Courbe de perte (Loss)\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "ax.plot(epochs_range, history_all[\"loss\"], label='Training Loss', linewidth=2)\n",
    "if history_all[\"val_loss\"]:\n",
    "    ax.plot(history_all[\"val_epochs\"], history_all[\"val_loss\"], label='Validation Loss', linewidth=2)\n",
    "ax.set_title(f\"{model_name} - Loss\")\n",
    "ax.set_xlabel(\"Epochs\"); ax.set_ylabel(\"Loss\")\n",
    "ax.legend(); ax.grid(True, linestyle='--', alpha=0.6)\n",
    "save_plot(fig, \"loss_curve.png\")\n",
    "plt.close(fig)\n",
    "\n",
    "# --- 2Ô∏è‚É£ Policy Accuracy\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "ax.plot(epochs_range, history_all[\"policy_acc\"], label='Policy Accuracy', linewidth=2)\n",
    "if history_all[\"val_policy_acc\"]:\n",
    "    ax.plot(history_all[\"val_epochs\"], history_all[\"val_policy_acc\"], '--', label='Val Policy Acc', linewidth=2)\n",
    "ax.set_title(f\"{model_name} - Policy Accuracy\")\n",
    "ax.set_xlabel(\"Epochs\"); ax.set_ylabel(\"Accuracy\")\n",
    "ax.legend(); ax.grid(True, linestyle='--', alpha=0.6)\n",
    "save_plot(fig, \"policy_accuracy.png\")\n",
    "plt.close(fig)\n",
    "\n",
    "# --- 3Ô∏è‚É£ Top-5 Accuracy\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "ax.plot(epochs_range, history_all[\"policy_top5\"], label='Top-5 Accuracy', linewidth=2)\n",
    "if history_all[\"val_policy_top5\"]:\n",
    "    ax.plot(history_all[\"val_epochs\"], history_all[\"val_policy_top5\"], '--', label='Val Top-5', linewidth=2)\n",
    "ax.set_title(f\"{model_name} - Top-5 Accuracy\")\n",
    "ax.set_xlabel(\"Epochs\"); ax.set_ylabel(\"Accuracy\")\n",
    "ax.legend(); ax.grid(True, linestyle='--', alpha=0.6)\n",
    "save_plot(fig, \"top5_accuracy.png\")\n",
    "plt.close(fig)\n",
    "\n",
    "# --- 4Ô∏è‚É£ Value MAE\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "ax.plot(epochs_range, history_all[\"value_mae\"], label='Value MAE', linewidth=2)\n",
    "if history_all[\"val_value_mae\"]:\n",
    "    ax.plot(history_all[\"val_epochs\"], history_all[\"val_value_mae\"], '--', label='Val Value MAE', linewidth=2)\n",
    "ax.set_title(f\"{model_name} - Value MAE\")\n",
    "ax.set_xlabel(\"Epochs\"); ax.set_ylabel(\"MAE\")\n",
    "ax.legend(); ax.grid(True, linestyle='--', alpha=0.6)\n",
    "save_plot(fig, \"value_mae.png\")\n",
    "plt.close(fig)\n",
    "\n",
    "# ================================================================\n",
    "# üìà LISSAGE DE LA POLICY ACCURACY\n",
    "# ================================================================\n",
    "def moving_average(data, window=5):\n",
    "    data = np.array(data)\n",
    "    if len(data) < window:\n",
    "        return data\n",
    "    return np.convolve(data, np.ones(window)/window, mode='valid')\n",
    "\n",
    "if history_all[\"policy_acc\"]:\n",
    "    smooth_acc = moving_average(history_all[\"policy_acc\"], 5)\n",
    "    epochs_smooth = np.arange(5, len(history_all[\"policy_acc\"]) + 1)\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    ax.plot(range(1, len(history_all[\"policy_acc\"]) + 1),\n",
    "            history_all[\"policy_acc\"], alpha=0.4, label='Brut', linewidth=1.5)\n",
    "    ax.plot(epochs_smooth, smooth_acc, color='red', linewidth=2.5,\n",
    "            label='Moyenne glissante (5)')\n",
    "    ax.set_title(f\"{model_name} - Policy Accuracy (Liss√©e)\")\n",
    "    ax.set_xlabel(\"Epochs\"); ax.set_ylabel(\"Accuracy\")\n",
    "    ax.legend(); ax.grid(True, linestyle='--', alpha=0.8)\n",
    "    save_plot(fig, \"policy_acc_smooth.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "# ================================================================\n",
    "# üßæ EXPORT DU R√âSUM√â DES M√âTRIQUES\n",
    "# ================================================================\n",
    "with open(os.path.join(output_dir, \"metrics_summary.txt\"), \"w\") as f:\n",
    "    f.write(f\"Model: {model_name}\\n\")\n",
    "    f.write(f\"Epochs: {len(history_all['loss'])}\\n\\n\")\n",
    "    if history_all[\"loss\"]:\n",
    "        f.write(f\"Final Training Loss: {history_all['loss'][-1]:.4f}\\n\")\n",
    "    if history_all[\"val_loss\"]:\n",
    "        f.write(f\"Final Validation Loss: {history_all['val_loss'][-1]:.4f}\\n\")\n",
    "    if history_all[\"policy_acc\"]:\n",
    "        f.write(f\"Final Policy Accuracy: {history_all['policy_acc'][-1]:.4f}\\n\")\n",
    "    if history_all[\"val_policy_acc\"]:\n",
    "        f.write(f\"Final Val Policy Accuracy: {history_all['val_policy_acc'][-1]:.4f}\\n\")\n",
    "    if history_all[\"value_mae\"]:\n",
    "        f.write(f\"Final Value MAE: {history_all['value_mae'][-1]:.4f}\\n\")\n",
    "    if history_all[\"val_value_mae\"]:\n",
    "        f.write(f\"Final Val Value MAE: {history_all['val_value_mae'][-1]:.4f}\\n\")\n",
    "\n",
    "print(f\"\\n‚úÖ Tous les fichiers et graphiques ont √©t√© enregistr√©s dans : {output_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
